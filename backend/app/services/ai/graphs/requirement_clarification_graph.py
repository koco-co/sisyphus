"""
éœ€æ±‚æ¾„æ¸…LangGraphçŠ¶æ€å›¾ - åŠŸèƒ½æµ‹è¯•æ¨¡å—
ä½¿ç”¨LangGraphå®ç°å¤šè½®éœ€æ±‚æ¾„æ¸…å¯¹è¯
"""
from typing import TypedDict, Annotated, Sequence, List, Dict, Any, Optional
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from sqlalchemy.ext.asyncio import AsyncSession

from app.services.ai.llm_service import MultiVendorLLMService


# å®šä¹‰çŠ¶æ€
class RequirementClarificationState(TypedDict):
    """éœ€æ±‚æ¾„æ¸…çŠ¶æ€"""
    # ç”¨æˆ·è¾“å…¥
    user_input: str

    # éœ€æ±‚ç›¸å…³
    requirement_document: str  # éœ€æ±‚æ–‡æ¡£å†…å®¹
    requirement_name: str  # éœ€æ±‚åç§°
    module_name: str  # æ¨¡å—åç§°

    # å¯¹è¯å†å²
    messages: Sequence[Dict[str, str]]  # [{"role": "user/assistant", "content": "..."}]

    # AIåˆ†æç»“æœ
    identified_issues: List[str]  # è¯†åˆ«åˆ°çš„é—®é¢˜
    risk_points: List[str]  # é£é™©ç‚¹
    suggestions: List[str]  # å»ºè®®

    # æ§åˆ¶æµç¨‹
    needs_clarification: bool  # æ˜¯å¦éœ€è¦ç»§ç»­æ¾„æ¸…
    is_complete: bool  # æ˜¯å¦å®Œæˆ
    question_count: int  # å·²æé—®è½®æ•°

    # ç”¨æˆ·å›å¤
    user_response: str  # ç”¨æˆ·å¯¹é—®é¢˜çš„å›å¤


# Promptæ¨¡æ¿
ANALYZE_REQUIREMENT_PROMPT = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº§å“ç»ç†å’Œæµ‹è¯•ä¸“å®¶ï¼Œè´Ÿè´£å¸®åŠ©ç”¨æˆ·å°†ç¢ç‰‡åŒ–çš„éœ€æ±‚è½¬åŒ–ä¸ºå®Œæ•´çš„éœ€æ±‚æ–‡æ¡£ã€‚

## ç”¨æˆ·å½“å‰éœ€æ±‚
{user_input}

## å¯¹è¯å†å²
{chat_history}

## ä½ çš„ä»»åŠ¡
1. åˆ†æç”¨æˆ·æä¾›çš„éœ€æ±‚æè¿°å’Œæˆªå›¾ï¼Œè¯†åˆ«ä¸å®Œæ•´ä¹‹å¤„
2. æå‡ºé’ˆå¯¹æ€§çš„é—®é¢˜æ¥æ¾„æ¸…éœ€æ±‚ï¼ŒåŒ…æ‹¬ï¼š
   - åŠŸèƒ½è¾¹ç•Œ (ä»€ä¹ˆæ˜¯in scope,ä»€ä¹ˆæ˜¯out of scope)
   - å¼‚å¸¸åœºæ™¯ (å¤±è´¥åœºæ™¯ã€è¾¹ç•Œæ¡ä»¶)
   - éåŠŸèƒ½éœ€æ±‚ (æ€§èƒ½ã€å®‰å…¨ã€å…¼å®¹æ€§)
   - é£é™©ç‚¹ (æ½œåœ¨çš„æŠ€æœ¯é£é™©ã€ä¸šåŠ¡é£é™©)
3. æ¯æ¬¡æé—®3-5ä¸ªé—®é¢˜ï¼Œé¿å…ä¸€æ¬¡æ€§æé—®å¤ªå¤š
4. ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€ï¼Œé¿å…è¿‡äºæŠ€æœ¯åŒ–

## å›ç­”æ ¼å¼ï¼ˆJSONï¼‰:
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼å›ç­”ï¼š
```json
{{
  "requirement_document": "æ ¹æ®å½“å‰ä¿¡æ¯æ•´ç†çš„éœ€æ±‚æ–‡æ¡£ï¼ˆMarkdownæ ¼å¼ï¼‰",
  "questions": [
    "é—®é¢˜1",
    "é—®é¢˜2",
    "é—®é¢˜3"
  ],
  "risk_points": [
    "é£é™©1",
    "é£é™©2"
  ],
  "suggestions": [
    "å»ºè®®1",
    "å»ºè®®2"
  ],
  "needs_clarification": true,
  "is_complete": false
}}
```

å¦‚æœéœ€æ±‚å·²ç»è¶³å¤Ÿå®Œæ•´ï¼Œå¯ä»¥å°†is_completeè®¾ä¸ºtrueï¼Œæ­¤æ—¶questionså¯ä»¥ä¸ºç©ºæ•°ç»„ã€‚

ç°åœ¨ï¼Œè¯·åˆ†æç”¨æˆ·çš„éœ€æ±‚å¹¶å¼€å§‹æé—®ã€‚
"""

UPDATE_REQUIREMENT_PROMPT = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº§å“ç»ç†å’Œæµ‹è¯•ä¸“å®¶ï¼Œè´Ÿè´£æ ¹æ®ç”¨æˆ·çš„å›å¤æ›´æ–°éœ€æ±‚æ–‡æ¡£ã€‚

## å½“å‰éœ€æ±‚æ–‡æ¡£
{requirement_document}

## ç”¨æˆ·æœ€æ–°å›å¤
{user_response}

## å¯¹è¯å†å²
{chat_history}

## ä½ çš„ä»»åŠ¡
1. æ ¹æ®ç”¨æˆ·çš„å›å¤æ›´æ–°éœ€æ±‚æ–‡æ¡£
2. ç»§ç»­æå‡ºéœ€è¦æ¾„æ¸…çš„é—®é¢˜
3. æ›´æ–°é£é™©ç‚¹å’Œå»ºè®®
4. åˆ¤æ–­éœ€æ±‚æ˜¯å¦å·²ç»å®Œæ•´

## å›ç­”æ ¼å¼ï¼ˆJSONï¼‰:
```json
{{
  "requirement_document": "æ›´æ–°åçš„éœ€æ±‚æ–‡æ¡£ï¼ˆMarkdownæ ¼å¼ï¼‰",
  "questions": [
    "æ–°é—®é¢˜1",
    "æ–°é—®é¢˜2"
  ],
  "risk_points": [
    "æ›´æ–°åçš„é£é™©1",
    "æ›´æ–°åçš„é£é™©2"
  ],
  "suggestions": [
    "æ›´æ–°åçš„å»ºè®®1"
  ],
  "needs_clarification": true,
  "is_complete": false
}}
```

å¦‚æœéœ€æ±‚å·²ç»å®Œæ•´ï¼Œå°†is_completeè®¾ä¸ºtrueï¼Œquestionså¯ä»¥ä¸ºç©ºæ•°ç»„ã€‚

ç°åœ¨ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„å›å¤æ›´æ–°éœ€æ±‚æ–‡æ¡£ã€‚
"""


class RequirementClarificationGraph:
    """éœ€æ±‚æ¾„æ¸…çŠ¶æ€å›¾"""

    def __init__(self, session: AsyncSession, user_id: int):
        """
        åˆå§‹åŒ–éœ€æ±‚æ¾„æ¸…å›¾

        Args:
            session: æ•°æ®åº“ä¼šè¯
            user_id: ç”¨æˆ·ID
        """
        self.session = session
        self.user_id = user_id
        self.checkpointer = MemorySaver()
        self.graph = None
        self._build_graph()

    def _build_graph(self):
        """æ„å»ºçŠ¶æ€å›¾"""
        # åˆ›å»ºçŠ¶æ€å›¾
        workflow = StateGraph(RequirementClarificationState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("analyze_requirement", self.analyze_requirement_node)
        workflow.add_node("update_requirement", self.update_requirement_node)
        workflow.add_node("generate_response", self.generate_response_node)

        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("analyze_requirement")

        # æ·»åŠ æ¡ä»¶è¾¹
        workflow.add_conditional_edges(
            "analyze_requirement",
            self.should_continue_clarification,
            {
                "continue": "update_requirement",
                "complete": "generate_response"
            }
        )

        workflow.add_conditional_edges(
            "update_requirement",
            self.should_continue_clarification,
            {
                "continue": "generate_response",
                "complete": "generate_response"
            }
        )

        workflow.add_edge("generate_response", END)

        # ç¼–è¯‘å›¾
        self.graph = workflow.compile(checkpointer=self.checkpointer)

    async def analyze_requirement_node(
        self,
        state: RequirementClarificationState
    ) -> Dict[str, Any]:
        """
        åˆ†æéœ€æ±‚èŠ‚ç‚¹

        é¦–æ¬¡è°ƒç”¨æ—¶åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œç”Ÿæˆåˆå§‹é—®é¢˜
        """
        print("ğŸ” [analyze_requirement] åˆ†æç”¨æˆ·éœ€æ±‚...")

        # è·å–ç”¨æˆ·çš„é»˜è®¤LLM
        llm = await MultiVendorLLMService.get_default_llm(self.session, self.user_id)
        if not llm:
            raise ValueError("ç”¨æˆ·æœªé…ç½®AIæœåŠ¡ï¼Œè¯·å…ˆåœ¨AIé…ç½®ä¸­æ·»åŠ ")

        # æ„å»ºprompt
        chat_history = self._format_chat_history(state.get("messages", []))

        prompt = ANALYZE_REQUIREMENT_PROMPT.format(
            user_input=state["user_input"],
            chat_history=chat_history
        )

        # è°ƒç”¨LLM
        response = await llm.ainvoke([
            {"role": "user", "content": prompt}
        ])

        # è§£æJSONå“åº”
        import json
        try:
            result = self._extract_json(response)

            return {
                "requirement_document": result.get("requirement_document", ""),
                "identified_issues": result.get("questions", []),
                "risk_points": result.get("risk_points", []),
                "suggestions": result.get("suggestions", []),
                "needs_clarification": result.get("needs_clarification", True),
                "is_complete": result.get("is_complete", False),
                "question_count": 1,
            }
        except Exception as e:
            print(f"è§£æLLMå“åº”å¤±è´¥: {e}")
            return {
                "requirement_document": state["user_input"],
                "identified_issues": ["éœ€æ±‚æè¿°ä¸å¤Ÿè¯¦ç»†ï¼Œè¯·æä¾›æ›´å¤šä¿¡æ¯"],
                "risk_points": [],
                "suggestions": [],
                "needs_clarification": True,
                "is_complete": False,
                "question_count": 1,
            }

    async def update_requirement_node(
        self,
        state: RequirementClarificationState
    ) -> Dict[str, Any]:
        """
        æ›´æ–°éœ€æ±‚èŠ‚ç‚¹

        æ ¹æ®ç”¨æˆ·çš„å›å¤æ›´æ–°éœ€æ±‚æ–‡æ¡£ï¼Œç»§ç»­æé—®
        """
        print("ğŸ“ [update_requirement] æ›´æ–°éœ€æ±‚æ–‡æ¡£...")

        # è·å–LLM
        llm = await MultiVendorLLMService.get_default_llm(self.session, self.user_id)

        # æ„å»ºprompt
        chat_history = self._format_chat_history(state.get("messages", []))

        prompt = UPDATE_REQUIREMENT_PROMPT.format(
            requirement_document=state.get("requirement_document", ""),
            user_response=state.get("user_response", ""),
            chat_history=chat_history
        )

        # è°ƒç”¨LLM
        response = await llm.ainvoke([
            {"role": "user", "content": prompt}
        ])

        # è§£æJSONå“åº”
        import json
        try:
            result = self._extract_json(response)

            return {
                "requirement_document": result.get("requirement_document", state.get("requirement_document", "")),
                "identified_issues": result.get("questions", []),
                "risk_points": result.get("risk_points", []),
                "suggestions": result.get("suggestions", []),
                "needs_clarification": result.get("needs_clarification", True),
                "is_complete": result.get("is_complete", False),
                "question_count": state.get("question_count", 0) + 1,
            }
        except Exception as e:
            print(f"è§£æLLMå“åº”å¤±è´¥: {e}")
            return {
                "needs_clarification": False,
                "is_complete": True,
                "question_count": state.get("question_count", 0) + 1,
            }

    async def generate_response_node(
        self,
        state: RequirementClarificationState
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆå“åº”èŠ‚ç‚¹

        å‘ç”¨æˆ·è¿”å›å½“å‰çš„é—®é¢˜å’Œéœ€æ±‚æ–‡æ¡£
        """
        print("ğŸ’¬ [generate_response] ç”Ÿæˆç”¨æˆ·å“åº”...")

        # æ„å»ºå“åº”æ–‡æœ¬
        if state.get("is_complete", False):
            response_content = f"""## âœ… éœ€æ±‚æ¾„æ¸…å®Œæˆ

### éœ€æ±‚æ–‡æ¡£
{state.get('requirement_document', '')}

### è¯†åˆ«åˆ°çš„é£é™©ç‚¹
{self._format_list(state.get('risk_points', []))}

### å»ºè®®
{self._format_list(state.get('suggestions', []))}

---

éœ€æ±‚æ¾„æ¸…å·²å®Œæˆï¼Œæ‚¨å¯ä»¥ç»§ç»­ä¸‹ä¸€æ­¥æ“ä½œã€‚"""
        else:
            response_content = f"""## ğŸ” éœ€æ±‚æ¾„æ¸…

### å½“å‰éœ€æ±‚æ–‡æ¡£
{state.get('requirement_document', '')}

### éœ€è¦ç¡®è®¤çš„é—®é¢˜
{self._format_list(state.get('identified_issues', []))}

### è¯†åˆ«åˆ°çš„é£é™©ç‚¹
{self._format_list(state.get('risk_points', []))}

### ğŸ’¡ å»ºè®®
{self._format_list(state.get('suggestions', []))}

---

è¯·å›ç­”ä»¥ä¸Šé—®é¢˜ï¼Œä»¥ä¾¿å®Œå–„éœ€æ±‚æ–‡æ¡£ã€‚"""

        # æ·»åŠ åˆ°æ¶ˆæ¯å†å²
        messages = list(state.get("messages", []))
        messages.append({
            "role": "assistant",
            "content": response_content
        })

        return {
            "messages": messages,
        }

    def should_continue_clarification(self, state: Dict[str, Any]) -> str:
        """
        åˆ¤æ–­æ˜¯å¦ç»§ç»­æ¾„æ¸…

        Returns:
            "continue" æˆ– "complete"
        """
        is_complete = state.get("is_complete", False)
        needs_clarification = state.get("needs_clarification", True)
        question_count = state.get("question_count", 0)

        # æœ€å¤šæé—®5è½®
        max_questions = 5

        if is_complete or not needs_clarification or question_count >= max_questions:
            return "complete"
        else:
            return "continue"

    def _format_chat_history(self, messages: List[Dict[str, str]]) -> str:
        """æ ¼å¼åŒ–èŠå¤©å†å²"""
        if not messages:
            return "ï¼ˆæ— å†å²å¯¹è¯ï¼‰"

        formatted = []
        for msg in messages[-5:]:  # åªæ˜¾ç¤ºæœ€è¿‘5æ¡
            role = "ç”¨æˆ·" if msg["role"] == "user" else "AIåŠ©æ‰‹"
            formatted.append(f"{role}: {msg['content']}")

        return "\n".join(formatted)

    def _format_list(self, items: List[str]) -> str:
        """æ ¼å¼åŒ–åˆ—è¡¨"""
        if not items:
            return "ï¼ˆæ— ï¼‰"

        return "\n".join([f"- {item}" for item in items])

    def _extract_json(self, response: str) -> Dict[str, Any]:
        """ä»LLMå“åº”ä¸­æå–JSON"""
        import json
        import re

        # å°è¯•ç›´æ¥è§£æ
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            pass

        # å°è¯•æå–ä»£ç å—ä¸­çš„JSON
        match = re.search(r'```json\n(.*?)\n```', response, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                pass

        # å°è¯•æå–JSONå¯¹è±¡
        match = re.search(r'\{.*\}', response, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(0))
            except json.JSONDecodeError:
                pass

        raise ValueError("æ— æ³•ä»å“åº”ä¸­æå–JSON")

    async def astream_chat(
        self,
        requirement_id: str,
        user_input: str,
        config: Optional[Dict[str, Any]] = None
    ):
        """
        æµå¼å¯¹è¯æ¥å£

        Args:
            requirement_id: éœ€æ±‚IDï¼ˆç”¨ä½œthread_idï¼‰
            user_input: ç”¨æˆ·è¾“å…¥
            config: é…ç½®å‚æ•°

        Yields:
            å“åº”ç‰‡æ®µ
        """
        if config is None:
            config = {"configurable": {"thread_id": requirement_id}}

        # åˆå§‹çŠ¶æ€
        initial_state = {
            "user_input": user_input,
            "requirement_document": "",
            "requirement_name": "",
            "module_name": "",
            "messages": [],
            "identified_issues": [],
            "risk_points": [],
            "suggestions": [],
            "needs_clarification": True,
            "is_complete": False,
            "question_count": 0,
            "user_response": "",
        }

        # è¿è¡ŒçŠ¶æ€å›¾
        async for event in self.graph.astream(initial_state, config):
            node_name = list(event.keys())[0]
            node_output = event[node_name]

            # å¦‚æœæ˜¯generate_responseèŠ‚ç‚¹ï¼Œyieldå“åº”
            if node_name == "generate_response" and "messages" in node_output:
                messages = node_output["messages"]
                if messages:
                    latest_message = messages[-1]["content"]
                    yield {
                        "type": "message",
                        "content": latest_message,
                        "is_complete": node_output.get("is_complete", False),
                    }


# ä½¿ç”¨ç¤ºä¾‹
"""
from app.services.ai.graphs.requirement_clarification_graph import RequirementClarificationGraph
from sqlalchemy.ext.asyncio import AsyncSession

async def example_usage():
    # åˆ›å»ºçŠ¶æ€å›¾å®ä¾‹
    graph = RequirementClarificationGraph(session, user_id=1)

    # å¼€å§‹éœ€æ±‚æ¾„æ¸…
    requirement_id = "req-123"
    user_input = "æˆ‘æƒ³åšä¸€ä¸ªç”¨æˆ·ç™»å½•åŠŸèƒ½"

    async for chunk in graph.astream_chat(requirement_id, user_input):
        if chunk["type"] == "message":
            print(chunk["content"])
            print(f"\nå®ŒæˆçŠ¶æ€: {chunk['is_complete']}")
"""
